{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following along with https://elitedatascience.com/keras-tutorial-deep-learning-in-python (MNIST dataset) -mt\n",
    "# attempting to fit model for cats and dogs kaggle dataset: https://www.kaggle.com/c/dogs-vs-cats -mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing numpy and setting a seed for reproductibility\n",
    "import numpy as np\n",
    "np.random.seed(123) # kinda irrelevant. not really using randomness. -mt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing standard keras modules/layers/utilities\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second attempt using: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html -mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory structure looks like:\n",
    "```\n",
    "small_training_dir/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Point to images for dataset creation (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_training_dir/train'\n",
    "validation_data_dir = '/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_training_dir/validation'\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 200\n",
    "epochs = 30\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape array shapes as needed (tensorflow vs theano)\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# img = load_img('/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_training_dir/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "# x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "# x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for batch in train_datagen.flow(x, batch_size=1,\n",
    "#                           save_to_dir='/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/preview', save_prefix='cat', save_format='jpeg'):\n",
    "#     i += 1\n",
    "#     if i > 20:\n",
    "#         break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 [==============================] - 47s 750ms/step - loss: 0.7361 - acc: 0.5252 - val_loss: 0.6859 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "62/62 [==============================] - 41s 659ms/step - loss: 0.6999 - acc: 0.5655 - val_loss: 0.6634 - val_acc: 0.5272\n",
      "Epoch 3/30\n",
      "62/62 [==============================] - 42s 682ms/step - loss: 0.6790 - acc: 0.5887 - val_loss: 0.6372 - val_acc: 0.6359\n",
      "Epoch 4/30\n",
      "62/62 [==============================] - 43s 700ms/step - loss: 0.6496 - acc: 0.6179 - val_loss: 0.6493 - val_acc: 0.5598\n",
      "Epoch 5/30\n",
      "62/62 [==============================] - 38s 620ms/step - loss: 0.6443 - acc: 0.6301 - val_loss: 0.6267 - val_acc: 0.6413\n",
      "Epoch 6/30\n",
      "62/62 [==============================] - 39s 621ms/step - loss: 0.6454 - acc: 0.6562 - val_loss: 0.5562 - val_acc: 0.7120\n",
      "Epoch 7/30\n",
      "62/62 [==============================] - 38s 607ms/step - loss: 0.6143 - acc: 0.6774 - val_loss: 0.6032 - val_acc: 0.7337\n",
      "Epoch 8/30\n",
      "62/62 [==============================] - 38s 619ms/step - loss: 0.6111 - acc: 0.6623 - val_loss: 0.6419 - val_acc: 0.7120\n",
      "Epoch 9/30\n",
      "62/62 [==============================] - 38s 618ms/step - loss: 0.5978 - acc: 0.6875 - val_loss: 0.5301 - val_acc: 0.7663\n",
      "Epoch 10/30\n",
      "62/62 [==============================] - 37s 601ms/step - loss: 0.6007 - acc: 0.6723 - val_loss: 0.6371 - val_acc: 0.7609\n",
      "Epoch 11/30\n",
      "62/62 [==============================] - 38s 613ms/step - loss: 0.5705 - acc: 0.7228 - val_loss: 0.5386 - val_acc: 0.8098\n",
      "Epoch 12/30\n",
      "62/62 [==============================] - 42s 684ms/step - loss: 0.5783 - acc: 0.6976 - val_loss: 0.4909 - val_acc: 0.7935\n",
      "Epoch 13/30\n",
      "62/62 [==============================] - 39s 622ms/step - loss: 0.5592 - acc: 0.7369 - val_loss: 0.5662 - val_acc: 0.7446\n",
      "Epoch 14/30\n",
      "62/62 [==============================] - 38s 608ms/step - loss: 0.5456 - acc: 0.7318 - val_loss: 0.5554 - val_acc: 0.7604\n",
      "Epoch 15/30\n",
      "62/62 [==============================] - 38s 613ms/step - loss: 0.5651 - acc: 0.7278 - val_loss: 0.5287 - val_acc: 0.7391\n",
      "Epoch 16/30\n",
      "62/62 [==============================] - 38s 609ms/step - loss: 0.5549 - acc: 0.7097 - val_loss: 0.5153 - val_acc: 0.8043\n",
      "Epoch 17/30\n",
      "62/62 [==============================] - 38s 607ms/step - loss: 0.5354 - acc: 0.7470 - val_loss: 0.5642 - val_acc: 0.7609\n",
      "Epoch 18/30\n",
      "62/62 [==============================] - 38s 607ms/step - loss: 0.5376 - acc: 0.7419 - val_loss: 0.4966 - val_acc: 0.8261\n",
      "Epoch 19/30\n",
      "62/62 [==============================] - 40s 638ms/step - loss: 0.5426 - acc: 0.7329 - val_loss: 0.5284 - val_acc: 0.7880\n",
      "Epoch 20/30\n",
      "62/62 [==============================] - 40s 646ms/step - loss: 0.5347 - acc: 0.7430 - val_loss: 0.5175 - val_acc: 0.7554\n",
      "Epoch 21/30\n",
      "62/62 [==============================] - 39s 637ms/step - loss: 0.5259 - acc: 0.7218 - val_loss: 0.5655 - val_acc: 0.7772\n",
      "Epoch 22/30\n",
      "62/62 [==============================] - 43s 689ms/step - loss: 0.5247 - acc: 0.7500 - val_loss: 0.6116 - val_acc: 0.7391\n",
      "Epoch 23/30\n",
      "62/62 [==============================] - 41s 667ms/step - loss: 0.5050 - acc: 0.7712 - val_loss: 0.4761 - val_acc: 0.7826\n",
      "Epoch 24/30\n",
      "62/62 [==============================] - 38s 617ms/step - loss: 0.5329 - acc: 0.7621 - val_loss: 0.5401 - val_acc: 0.7772\n",
      "Epoch 25/30\n",
      "62/62 [==============================] - 38s 617ms/step - loss: 0.5365 - acc: 0.7429 - val_loss: 0.5018 - val_acc: 0.7663\n",
      "Epoch 26/30\n",
      "62/62 [==============================] - 38s 613ms/step - loss: 0.5200 - acc: 0.7500 - val_loss: 0.5037 - val_acc: 0.7554\n",
      "Epoch 27/30\n",
      "62/62 [==============================] - 38s 618ms/step - loss: 0.5088 - acc: 0.7591 - val_loss: 0.5420 - val_acc: 0.7812\n",
      "Epoch 28/30\n",
      "62/62 [==============================] - 39s 621ms/step - loss: 0.4966 - acc: 0.7520 - val_loss: 0.4656 - val_acc: 0.7880\n",
      "Epoch 29/30\n",
      "62/62 [==============================] - 39s 626ms/step - loss: 0.5257 - acc: 0.7429 - val_loss: 0.5203 - val_acc: 0.7663\n",
      "Epoch 30/30\n",
      "62/62 [==============================] - 38s 618ms/step - loss: 0.4997 - acc: 0.7631 - val_loss: 0.6933 - val_acc: 0.7065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.5: Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('catsndogs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.9: Reload model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('catsndogs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Test things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cropResizeImages(img):\n",
    "    width, height = img.size\n",
    "\n",
    "    if width > height:\n",
    "        left = (width - height)/2\n",
    "        top = (height - height)/2\n",
    "        right = (width + height)/2\n",
    "        bottom = (height + height)/2\n",
    "        \n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        img = img.resize([150,150])\n",
    "    \n",
    "    elif height > width:\n",
    "        left = (width - width)/2\n",
    "        top = (height - width)/2\n",
    "        right = (width + width)/2\n",
    "        bottom = (height + width)/2\n",
    "        \n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        img = img.resize([150,150])\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_img = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_testing_dir/11.jpg\" # cat\n",
    "# test_img = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_testing_dir/27.jpg\" # dog\n",
    "# test_img = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_testing_dir/82.jpg\" # cat\n",
    "test_img = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_training_dir/train/cats/cat.100.jpg\" #cat, training\n",
    "# test_img = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_training_dir/train/dogs/dog.100.jpg\" #dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img = load_img(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_cropped = cropResizeImages(test_img)\n",
    "test_img_in = np.array(test_img_cropped)\n",
    "plt.imshow(test_img_in);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(test_img_in[np.newaxis, :, :, :])\n",
    "probs = model.predict_proba(test_img_in[np.newaxis, :, :, :])\n",
    "print(preds, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Run images through model and sort according to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaning_dir = \"/Users/minhgeneralassembly/Downloads/kaggle_catsanddogs/small_testing_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(cleaning_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        with open(os.path.join(cleaning_dir, filename)) as f:\n",
    "            content = f.read()\n",
    "            print con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Preprocess class labels for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at the shape of our class label data\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expecting 10 classes (one for each digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at labels for first 10 training samples\n",
    "print y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess class labels\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting shape\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using a pre-defined / proven architecture for this\n",
    "# declare sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare CNN input layer\n",
    "# added dim_ordering='th' since i'm using tensorflow and it has different ways of ordering dimensions\n",
    "# model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), dim_ordering='th'))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format=\"channels_first\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding more layers to model\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully connected dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.5. Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create %20 validation set\n",
    "X_val = X_train[:(len(X_train)/5)]\n",
    "partial_X_train = X_train[(len(X_train)/5):]\n",
    "\n",
    "Y_val = Y_train[:(len(X_train)/5)]\n",
    "partial_Y_train = Y_train[(len(X_train)/5):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Fit model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normal\n",
    "# history = model.fit(X_train, Y_train, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_X_train,\n",
    "                    partial_Y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Unshape image to look at, then predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = X_test[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view test_image (undoing the transformation on a copy)\n",
    "test_image_in = test_image\n",
    "test_image_in *= 255\n",
    "test_image_in = test_image_in.reshape(28, 28)\n",
    "plt.imshow(test_image_in, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(test_image[np.newaxis, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_image[np.newaxis, :, :, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
